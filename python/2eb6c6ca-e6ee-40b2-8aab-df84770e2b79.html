<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Python 爬虫 | Janvy's blog</title><meta name="keywords" content="Python,爬虫"><meta name="author" content="Janvy Sun"><meta name="copyright" content="Janvy Sun"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="网络爬虫（又称为网页蜘蛛，网络机器人，在 FOAF 社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。"><meta property="og:type" content="article"><meta property="og:title" content="Python 爬虫"><meta property="og:url" content="https://janvysun.github.io/python/2eb6c6ca-e6ee-40b2-8aab-df84770e2b79.html"><meta property="og:site_name" content="Janvy&#39;s blog"><meta property="og:description" content="网络爬虫（又称为网页蜘蛛，网络机器人，在 FOAF 社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://janvysun.github.io/images/hexo.png"><meta property="article:published_time" content="2019-04-14T16:00:00.000Z"><meta property="article:modified_time" content="2019-04-14T16:00:00.000Z"><meta property="article:author" content="Janvy Sun"><meta property="article:tag" content="Python"><meta property="article:tag" content="爬虫"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://janvysun.github.io/images/hexo.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://janvysun.github.io/python/2eb6c6ca-e6ee-40b2-8aab-df84770e2b79"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: [object Object]
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Python 爬虫",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2019-04-15 00:00:00"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 6.1.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">153</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">47</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background:linear-gradient(20deg,#0062be,#925696,#cc426e,#fb0347)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Janvy's blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python 爬虫</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-04-14T16:00:00.000Z" title="发表于 2019-04-15 00:00:00">2019-04-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2019-04-14T16:00:00.000Z" title="更新于 2019-04-15 00:00:00">2019-04-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>42分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Python 爬虫"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="爬虫基本概念"><a href="#爬虫基本概念" class="headerlink" title="爬虫基本概念"></a>爬虫基本概念</h2><p>网络爬虫(又被称之为网页蜘蛛，网络机器人)，就是 <strong>模拟客户端发送网络请求，接收请求响应，按照一定的规则自动地抓取互联网信息的程序</strong>。</p><p>一般来说，<strong>只要是浏览器能做的事情，爬虫都能做</strong>。</p><h3 id="爬虫的流程"><a href="#爬虫的流程" class="headerlink" title="爬虫的流程"></a>爬虫的流程</h3><ul><li>url—&gt;发送请求，获取响应—&gt;提取数据—&gt;保存</li><li>发送请求，获取响应—&gt;提取url</li></ul><h2 id="Requests-模块"><a href="#Requests-模块" class="headerlink" title="Requests 模块"></a>Requests 模块</h2><p>该模块作用：发送网络请求，返回响应数据</p><p><a target="_blank" rel="noopener" href="http://docs.python-requests.org/zh_CN/latest/index.html">中文文档</a></p><h3 id="Requests-基本操作"><a href="#Requests-基本操作" class="headerlink" title="Requests 基本操作"></a>Requests 基本操作</h3><p>要使用 Requests 模块，首先需要导入 Requests: <code>import Requests</code></p><p>然后就可以获取某个网页，如用 get 方法获取百度网： <code>response = requests.get(&quot;https://www.baidu.com&quot;)</code></p><p>现在，我们有一个名为 response 的 Response 对象。我们可以从这个对象中获取所有我们想要的信息。</p><p>response 常用属性</p><ul><li><strong><code>response.text</code></strong><br>获取网页 HTML 源码(str类型)<br>解码类型：根据HTTP头部对响应的编码作出有根据的推测，推测的文本编码<br>可以修改编码方式：<code>response.encoding=&quot;utf8&quot;</code></li><li><strong><code>response.content</code></strong><br>获取网页数据(byte 类型)<br>可以使用 decode 方法进行解码：<code>response.content.decode(&quot;utf8&quot;)</code></li><li><strong><code>response.status_code</code></strong><br>获取响应状态码</li><li><strong><code>response.request.headers</code></strong><br>获取请求头</li><li><strong><code>response.headers</code></strong><br>获取响应头</li></ul><h3 id="发送带-header-的请求"><a href="#发送带-header-的请求" class="headerlink" title="发送带 header 的请求"></a>发送带 header 的请求</h3><p>作用：模拟浏览器，欺骗服务器，获取和浏览器一致的内容<br>header 形式：字典<br>如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用法：<code>request.get(url, headers=HEADERS)</code></p><h3 id="发送带参数的请求"><a href="#发送带参数的请求" class="headerlink" title="发送带参数的请求"></a>发送带参数的请求</h3><p>参数就是 url 后面”?”之后所带的内容，参数的形式也是字典<br>如：<code>kw = &#123;&#39;wd&#39;:&#39;python&#39;&#125;</code><br>用法：<code>request.get(url,params=PARAMS)</code></p><p>案例如下：爬取任意贴吧，保存网页到本地</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TiebaSpider</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name</span>):</span><br><span class="line">        self.tieba_name = name</span><br><span class="line">        self.url_temp = <span class="string">&quot;https://tieba.baidu.com/f?kw=&quot;</span> + self.tieba_name + <span class="string">&quot;&amp;ie=utf-8&amp;pn=&#123;&#125;&quot;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;User_Agent&quot;</span>:<span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_url_list</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [self.url_temp.<span class="built_in">format</span>(i*<span class="number">50</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="built_in">print</span>(url)</span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> response.content.decode()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_html</span>(<span class="params">self, html_str, page_num</span>):</span><br><span class="line">        file_path = <span class="string">&quot;&#123;&#125;_第&#123;&#125;页.html&quot;</span>.<span class="built_in">format</span>(self.tieba_name, page_num)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(html_str)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 1.构造 URL 列表</span></span><br><span class="line">        url_list = self.get_url_list()</span><br><span class="line">        <span class="comment"># 2.遍历，发送请求，获取响应</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">            html_str = self.parse_url(url)</span><br><span class="line">            <span class="comment"># 3.保存</span></span><br><span class="line">            page_num = url_list.index(url) + <span class="number">1</span></span><br><span class="line">            self.save_html(html_str, page_num)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    tieba_spider = TiebaSpider(<span class="string">&quot;李毅&quot;</span>)</span><br><span class="line">    tieba_spider.run()</span><br></pre></td></tr></table></figure><h3 id="发送-POST-请求"><a href="#发送-POST-请求" class="headerlink" title="发送 POST 请求"></a>发送 POST 请求</h3><p>当我们需要登录或者注册的时候，这我们就要发送 POST 请求</p><p>爬虫也同样可以模拟浏览器发送 POST 请求</p><p>发送 POST 请求需要一个参数 data，该参数为字典类型</p><p>requests 类中的 post 方法可以发送 POST 请求，用法如下：<code>requests.post(URL, data=DATA)</code></p><p>案例：爬取有道翻译：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Translate</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, string</span>):</span><br><span class="line">        self.url = <span class="string">&quot;http://m.youdao.com/translate&quot;</span></span><br><span class="line">        self.string = string</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (iPhone; CPU iPhone OS 10_3_1 like Mac OS X) AppleWebKit/603.1.30 (KHTML, like Gecko) Version/10.0 Mobile/14E304 Safari/602.1&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">send_post</span>(<span class="params">self</span>):</span><br><span class="line">        post_data = &#123;</span><br><span class="line">            <span class="string">&#x27;inputtext&#x27;</span>:self.string,</span><br><span class="line">            <span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;AUTO&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> requests.post(self.url, data=post_data, headers=self.headers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                response = self.send_post()</span><br><span class="line">                ret = re.search(<span class="string">r&#x27;&lt;ul id=&quot;translateResult&quot;&gt;\s+&lt;li&gt;(.*)&lt;/li&gt;&#x27;</span>, response.content.decode()).group(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">except</span> AttributeError:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    fanyi = Translate(sys.argv[<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(fanyi.run())</span><br></pre></td></tr></table></figure><h3 id="使用代理"><a href="#使用代理" class="headerlink" title="使用代理"></a>使用代理</h3><p>我们使用代理的目的是为了让服务器以为不是同一个客户在请求，也可以防止我们的真实IP被泄露<br>使用代理需要一个参数proxies，该参数为字典类型，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>:<span class="string">&quot;http://12.34.56.78:9000&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;https://12.34.56.78:9000&#x27;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用法如下：<br><code>requests.get(url, proxies=PROXIES)</code></p><p>我们使用代理的时候可以准备一堆的IP地址，组成IP池，要用的时候随机选择一个IP来用<br>在使用代理的时候，有些代理可以速度很慢，这时候需要检查IP地址的可用性：<br><strong>可以在使用requests的方法的时候添加超时参数，判断IP地址的质量</strong><br><strong>可以在线检测IP质量</strong></p><h3 id="Cookies-和-Session"><a href="#Cookies-和-Session" class="headerlink" title="Cookies 和 Session"></a>Cookies 和 Session</h3><p>Cookies和Session的区别：</p><blockquote><p>cookie数据存放在<strong>客户的浏览器</strong>上，session数据放在<strong>服务器</strong>上。<br>cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗。<br>session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能。<br>单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。</p></blockquote><p>requests 提供了一个叫做session类，来实现客户端和服务端的会话保持<br>使用方法：先实例化一个session对象，然后让session发送get或者post请求<br>session中的get和post方法的使用方式和requests中的get和post是一样的<br>如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session = requests.session()</span><br><span class="line">response = session.get(url,headers)</span><br></pre></td></tr></table></figure><p>如果要使用session请求登陆后的页面，<strong>需要发送两次请求</strong>：</p><blockquote><p>先使用session发送请求，登录对网站，把cookie保存在session中<br>再使用session请求登陆之后才能访问的网站，session能够自动的携带登录成功时保存在其中的cookie，进行请求</p></blockquote><ul><li><strong>Cookie 的处理</strong><br>当我们得到一串 cookies 的时候，我们需要将 cookies 以列表的方式存储<br>这时候需要用到列表推导式：<br>如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cookies = <span class="string">&quot;&quot;&quot;anonymid=j3jxk555-nrn0wh; _r01_=1;</span></span><br><span class="line"><span class="string">    _ga=GA1.2.1274811859.1497951251;</span></span><br><span class="line"><span class="string">    _de=BF09EE3A28DED52E6B65F6A4705D973F1383380866D39FF5;</span></span><br><span class="line"><span class="string">    ln_uact=mr_mao_hacker@163.com; depovince=BJ;</span></span><br><span class="line"><span class="string">    jebecookies=54f5d0fd-9299-4bb4-801c-eefa4fd3012b|||||;</span></span><br><span class="line"><span class="string">    JSESSIONID=abcI6TfWH4N4t_aWJnvdw;</span></span><br><span class="line"><span class="string">    ick_login=4be198ce-1f9c-4eab-971d-48abfda70a50;</span></span><br><span class="line"><span class="string">    p=0cbee3304bce1ede82a56e901916d0949; first_login_flag=1;</span></span><br><span class="line"><span class="string">    ln_hurl=http://hdn.xnimg.cn/photos/hdn421/20171230/1635/main_JQzq_ae7b0000a8791986.jpg;</span></span><br><span class="line"><span class="string">    t=79bdd322e760beae79c0b511b8c92a6b9;</span></span><br><span class="line"><span class="string">    societyguester=79bdd322e760beae79c0b511b8c92a6b9; id=327550029;</span></span><br><span class="line"><span class="string">    xnsid=2ac9a5d8; loginfrom=syshome; ch_id=10016; wp_fold=0&quot;&quot;&quot;</span></span><br><span class="line">cookies = &#123;i.split(<span class="string">&quot;=&quot;</span>)[<span class="number">0</span>]:i.split(<span class="string">&quot;=&quot;</span>)[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> cookies.split(<span class="string">&quot;; &quot;</span>)&#125;</span><br></pre></td></tr></table></figure>在 requests 模块中的 get 和 post 方法中可以添加 cookies，也可以在 headers 中添加 cookies，如下：<br><code>get(url, headers=HEADERS, cookies=COOKIES)</code></li></ul><h3 id="requests-模块的小技巧"><a href="#requests-模块的小技巧" class="headerlink" title="requests 模块的小技巧"></a>requests 模块的小技巧</h3><ul><li><p><strong>cookies 对象和字典的转换</strong><br>我们可以在请求的响应中拿到 cookies，这是一个对象，我们可以将它转化为一个字典，或者将一个字典转化为一个 cookies 对象<br>如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">requests.utils.dict_from_cookiejar(COOKIES)</span><br><span class="line">requests.utils.cookiejar_from_dict(DICT)</span><br></pre></td></tr></table></figure></li><li><p><strong>URL地址的编码和解码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">requests.utils.unquote(URL)  <span class="comment"># URL的解码</span></span><br><span class="line">requests.utils.quote(URL)    <span class="comment"># URL的编码</span></span><br></pre></td></tr></table></figure></li><li><p><strong>SSL证书的验证</strong><br>有的时候我们请求https协议的网址的时候，如果该网站的SSL证书有问题，程序就会报一个SSLError的异常，此时我们就需要取消SSL证书的认证<br>用法如下：<br><code>resquests.get(URL, verify=False)</code><br>这样使用的时候会有一个警告，说明该链接不安全，但是并不影响程序的运行</p></li><li><p><strong>设置超时参数</strong><br>我们可以在方法中添加一个超时参数，用法如下：<br><code>requests.get(URL, timeout=NUM)</code></p></li></ul><h3 id="自制请求模块"><a href="#自制请求模块" class="headerlink" title="自制请求模块"></a>自制请求模块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> retrying <span class="keyword">import</span> retry</span><br><span class="line"></span><br><span class="line">headers = &#123; </span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@retry(<span class="params">stop_max_attempt_number=<span class="number">3</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_parse_url</span>(<span class="params">url, method, data, proxies</span>):</span><br><span class="line">    <span class="keyword">if</span> method = <span class="string">&quot;POST&quot;</span>:</span><br><span class="line">        response = requests.post(url, data=data, headers=headers, proxies=proxies)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        response = requests.get(url, headers=headers, timeout=<span class="number">3</span>, proxies=proxies)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> response.status_code == <span class="number">200</span> </span><br><span class="line">    <span class="keyword">return</span> response.content.decode()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">url, method=<span class="string">&quot;GET&quot;</span>, data=<span class="literal">None</span>, proxies=&#123;&#125;</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        html_str = _parse_url(url, method, data, proxies)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        html_str = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> html_str</span><br></pre></td></tr></table></figure><h2 id="数据提取"><a href="#数据提取" class="headerlink" title="数据提取"></a>数据提取</h2><p>简单来说，数据提取就是<strong>从响应中获取我们想要的数据</strong>的过程<br>数据通常分为两类：<strong>非结构化数据和结构化数据</strong></p><ul><li><p><strong>非结构化数据</strong><br>如：HTML，通常用正则表达式和xpath来处理</p></li><li><p><strong>结构化数据</strong><br>如：json、xml等，通常转化为Python数据类型来处理</p></li></ul><h3 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h3><blockquote><p>JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式，它使得人们很容易的进行阅读和编写<br>同时也方便了机器进行解析和生成<br>适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。</p></blockquote><p>由于把json数据转化为python内建数据类型很简单，所以爬虫中，如果我们能够找到返回json数据的URL，就会尽量使用这种URL</p><p>在python中有个模块专门用于对json字符串进行处理，在使用的时候需要导入json模块：<br><code>import json</code></p><ul><li><p><strong>loads</strong><br>这个方法用于将json字符串转换为python数据类型<br>用法如下：<br><code>json.loads(JSON_str)</code><br>返回值是Python数据类型(列表或者字典)</p></li><li><p><strong>dumps</strong><br>这个方法用于将Python数据类型转换为json字符串<br>用法如下：<br><code>json.dumps(PYTHON_TYPE, ensure_ascii=True, indent=NUM)</code><br>该方法有2个比较常用的参数:</p><ul><li><strong>ensure_ascii</strong><br>指定是否保存为ascii格式，默认为True，该为False则会以utf-8的形式保存</li><li><strong>indent</strong><br>保存时用较为美观的样式，该值为缩进值，通常指定为2或4</li></ul></li><li><p><strong>load</strong><br>该方法用于将包含json的类文件对象中的数据提取为Python数据类型<br>类文件对象：</p><blockquote><p>具有read()或者write()方法的对象就是类文件对象<br>如f &#x3D; open(“a.txt”,”r”) f就是类文件对象</p></blockquote><p>用法和loads相同<br>如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用json.load提取类文件对象中的数据</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test.json&quot;</span>,<span class="string">&quot;r&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    ret = json.load(f)</span><br></pre></td></tr></table></figure></li><li><p><strong>dump</strong><br>该方法用于将Python数据类型转化为类文件对象，用法和dumps类似，需要多接收一个要转换的python类型变量<br>如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># json.dump能够把python类型放入类文件对象中</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test.json&quot;</span>,<span class="string">&quot;w&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(ret,f,ensure_ascii=<span class="literal">False</span>,indent=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></li></ul><p><strong>注意</strong>：</p><blockquote><p>json中的字符串都是双引号引起来的，如果不是双引号，则需要对其进行转换，如：</p><ul><li>eval：能实现简单的字符串和python类型的转化</li><li>replace：把单引号替换为双引号</li></ul><p>如果要往一个文件中写入多个json串，则其不再是一个json串，不能直接读取，我们可以一行写一个json串，按照行来读取</p></blockquote><p>爬取豆瓣如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DouBanSpider</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url_temp_list = [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;url_temp&quot;</span>: <span class="string">&quot;https://m.douban.com/filter_tv_american_hot/items?start=&#123;&#125;&amp;count=18&amp;loc_id=108288&quot;</span>,</span><br><span class="line">                <span class="string">&quot;country&quot;</span>: <span class="string">&quot;US&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;url_temp&quot;</span>: <span class="string">&quot;https://m.douban.com/filter_tv_english_hot/items?start=&#123;&#125;&amp;count=18&amp;loc_id=108288&quot;</span>,</span><br><span class="line">                <span class="string">&quot;country&quot;</span>: <span class="string">&quot;UK&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;url_temp&quot;</span>: <span class="string">&quot;https://m.douban.com/filter_tv_domestic_hot/items?start=&#123;&#125;&amp;count=18&amp;loc_id=108288&quot;</span>,</span><br><span class="line">                <span class="string">&quot;country&quot;</span>: <span class="string">&quot;CN&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>:</span><br><span class="line">            <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self, url</span>):  <span class="comment"># 发送请求，获取响应</span></span><br><span class="line">        <span class="built_in">print</span>(url)</span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> response.content.decode()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_content_list</span>(<span class="params">self, json_str</span>):  <span class="comment"># 提取请求数据</span></span><br><span class="line">        dict_ret = json.loads(json_str)</span><br><span class="line">        content_list = dict_ret[<span class="string">&quot;subject_collection_items&quot;</span>]</span><br><span class="line">        <span class="keyword">return</span> content_list</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_content_list</span>(<span class="params">self, content_list, country</span>):  <span class="comment"># 保存数据</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test.txt&quot;</span>, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> content_list:</span><br><span class="line">                content[<span class="string">&#x27;country&#x27;</span>] = country</span><br><span class="line">                f.write(json.dumps(content, ensure_ascii=<span class="literal">False</span>))</span><br><span class="line">                f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> url_temp <span class="keyword">in</span> self.url_temp_list:</span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                <span class="comment"># 1.请求起始页面</span></span><br><span class="line">                url = url_temp[<span class="string">&#x27;url_temp&#x27;</span>].<span class="built_in">format</span>(num)</span><br><span class="line">                <span class="comment"># 2. 发送请求，获取响应</span></span><br><span class="line">                json_str = self.parse_url(url)</span><br><span class="line">                <span class="comment"># 3.提取数据</span></span><br><span class="line">                content_list = self.get_content_list(json_str)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 4.保存</span></span><br><span class="line">                self.save_content_list(content_list, url_temp[<span class="string">&#x27;country&#x27;</span>])</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(content_list) &lt; <span class="number">18</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 5.构造下一页url地址</span></span><br><span class="line">                num += <span class="number">18</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    douban = DouBanSpider()</span><br><span class="line">    douban.run()</span><br></pre></td></tr></table></figure><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><blockquote><p>正则表达式就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个”规则字符串”<br>这个<strong>规则字符串</strong>用来表达对字符串的一种<strong>过滤</strong>逻辑。</p></blockquote><p>常用的正则表达式的方法：<br>编译：re.compile()<br>从头匹配：pattern.match()<br>找一个：pattern.search()<br>找所有(<strong>常用</strong>)：pattern.findall()<br>替换：pattern.sub()</p><p>使用正则表达式的注意点：</p><blockquote><p><code>.</code>默认不能匹配<code>\n</code>，但是在匹配的时候加上<code>re.S</code>或者<code>re.DOTALL</code>就可以匹配<br><code>re.findall</code>方法中，如果模式没有分组则匹配模式，如果有分组则匹配分组内容</p></blockquote><p>内涵段子爬虫：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Neihan</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.start_url = <span class="string">&#x27;http://neihanshequ.com/&#x27;</span></span><br><span class="line">        self.next_url_temp = <span class="string">&#x27;http://neihanshequ.com/joke?is_json=1&amp;app_name=neihanshequ_web&amp;max_time=&#123;&#125;&#x27;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;User-Agent&quot;</span>:</span><br><span class="line">            <span class="string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="built_in">print</span>(url)</span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> response.content.decode()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_first_page_content_list</span>(<span class="params">self, html_str</span>):</span><br><span class="line">        content_list = re.findall(<span class="string">r&#x27;&lt;h1 class=&quot;title&quot;&gt;.*?&lt;p&gt;(.*?)&lt;/p&gt;&#x27;</span>, html_str, re.S)</span><br><span class="line">        max_time = re.findall(<span class="string">&quot;max_time: &#x27;(.*?)&#x27;,&quot;</span>, html_str)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> content_list,max_time</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_content_list</span>(<span class="params">self, content_list</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;neihan.txt&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> content_list:</span><br><span class="line">                f.write(json.dumps(content, ensure_ascii=<span class="literal">False</span>))</span><br><span class="line">                f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_content_list</span>(<span class="params">self, json_str</span>):</span><br><span class="line">        dict_ret = json.loads(json_str)</span><br><span class="line">        data = dict_ret[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">        content_list = [i[<span class="string">&#x27;group&#x27;</span>][<span class="string">&#x27;max_time&#x27;</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">        max_time = dict_ret[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;max_time&#x27;</span>]</span><br><span class="line">        has_more = dict_ret[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;has_more&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> content_list,max_time,has_more</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 1.start_url</span></span><br><span class="line">        <span class="comment"># 2.发送请求，获取响应</span></span><br><span class="line">        html_str = self.parse_url(self.start_url)</span><br><span class="line">        <span class="comment"># 3.提取数据</span></span><br><span class="line">        content_list,maxtime = self.get_first_page_content_list(html_str)</span><br><span class="line">        <span class="comment"># 4.保存</span></span><br><span class="line">        self.save_content_list(content_list)</span><br><span class="line">        has_more = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> has_more:</span><br><span class="line">            next_url = self.next_url_temp.<span class="built_in">format</span>(max_time)</span><br><span class="line">            json_str = self.parse_url(next_url)</span><br><span class="line">            content_list,max_time,has_more = self.get_content_list(json_str)</span><br><span class="line">            self.save_content_list(content_list)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    neihan = Neihan()</span><br><span class="line">    neihan.run()</span><br></pre></td></tr></table></figure><h3 id="xpath"><a href="#xpath" class="headerlink" title="xpath"></a>xpath</h3><p>什么是XPATH：</p><blockquote><p>XPath(XML Path Language)是一门在<strong>HTML\XML文档</strong>中查找信息的语言<br>可用来在HTML\XML文档中<strong>对元素和属性进行遍历</strong>。</p></blockquote><p>我们可以在Chrome浏览器中使用插件<strong>XPath Helper</strong>来帮助我们学习Xpath语法<br>使用Xpath Helper插件选择标签的时候，选中的标签会自动添加一个属性<strong>class&#x3D;”xh-highlight”</strong><br><strong>注意</strong>：</p><blockquote><p>使用xpath helper是从element中提取的数据<br>但是我们的爬虫获取的是url对应的响应，往往和elements不一样</p></blockquote><p>XPath 使用路径表达式来选取<strong>HTML\XML文档</strong>中的节点或者节点集(这里的节点也可以是标签)。<br>这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。</p><p>xpath语法：</p><table><thead><tr><th align="center">表达式</th><th>描述</th></tr></thead><tbody><tr><td align="center">&#x2F;</td><td>从根节点选取</td></tr><tr><td align="center">&#x2F;&#x2F;</td><td>从任意位置选取</td></tr><tr><td align="center">NODENAME</td><td>选取此<strong>节点\标签</strong>的所有子<strong>节点\标签</strong></td></tr><tr><td align="center">.</td><td>选取当前节点</td></tr><tr><td align="center">..</td><td>选取当前节点的父节点</td></tr><tr><td align="center">@</td><td>选取属性，也可用于筛选属性</td></tr><tr><td align="center">*</td><td>通配符，用于匹配所有节点</td></tr><tr><td align="center">node()</td><td>和<code>*</code>一样，用于匹配所有节点</td></tr><tr><td align="center">@*</td><td>匹配所有属性值</td></tr><tr><td align="center">&#96;</td><td>&#96;</td></tr></tbody></table><p>xpath案例：</p><table><thead><tr><th align="center">路径表达式</th><th>结果</th></tr></thead><tbody><tr><td align="center">&#x2F;bookstore&#x2F;book[1]</td><td>选取属于bookstore子元素的第一个book元素</td></tr><tr><td align="center">&#x2F;bookstore&#x2F;book[last()]</td><td>选取属于bookstore子元素的最后一个book元素</td></tr><tr><td align="center">&#x2F;bookstore&#x2F;book[last()-1]</td><td>选取属于bookstore子元素的倒数第二个book元素</td></tr><tr><td align="center">bookstore&#x2F;book[position()&lt;3]</td><td>选取最前面两个属于bookstore子元素的book元素</td></tr><tr><td align="center">&#x2F;&#x2F;title[@lang]</td><td>选取所有拥有名为lang属性的title元素</td></tr><tr><td align="center">&#x2F;&#x2F;title[@lang&#x3D;”eng”]</td><td>选取所有lang属性值为’eng’的title元素</td></tr></tbody></table><p>常用：</p><ul><li><p><strong>获取文本</strong></p><ul><li><code>a/text()</code> 表示获取a下的文本</li><li><code>a//text()</code> 表示获取a下的所有标签的文本</li><li><code>//a[text()=&#39;下一页&#39;]</code> 表示选择文本为下一页三个字的a标签</li></ul></li><li><p><strong>@符号</strong></p><ul><li><code>a/@href</code> 表示获取a标签的href属性值</li><li><code>//ul[@id=&quot;detail-list&quot;]</code> 表示获取id为”detail-list”的ul标签</li></ul></li><li><p><strong>&#x2F;&#x2F;</strong></p><ul><li>在xpath最前面表示从当前html中任意位置开始选择</li><li><code>li//a</code> 表示的是li下任何一个a标签</li></ul></li><li><p><strong>包含(contains)</strong><br><code>//div[contains(@class, &#39;i&#39;)]</code>表示class属性包含’i’的所有div标签</p></li></ul><h3 id="lxml"><a href="#lxml" class="headerlink" title="lxml"></a>lxml</h3><blockquote><p>lxml是一款高性能的Python<strong>HTML&#x2F;XML</strong>解析器<br>我们可以利用其中的XPath，来快速的定位特定元素以及获取节点信息</p></blockquote><p>在使用之前需要导入lxml中的etree库：<br><code>from lxml import etree</code><br>然后我们可以利用<code>etree.HTML</code>，将字符串转化为<strong>Element对象</strong>:<br><code>html = etree.HTML(HTML_STR)</code><br><strong>注意：lxml可以自动修正html代码</strong><br>Element对象具有Xpath的方法</p><p>案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    &lt;div&gt;&lt;ul&gt; </span></span><br><span class="line"><span class="string">    &lt;li class=&quot;item-1&quot;&gt;&lt;a&gt;first item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">    &lt;/ul&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">html = etree.HTML(text)</span><br><span class="line"><span class="comment">#查看element对象中包含的字符串</span></span><br><span class="line"><span class="built_in">print</span>(etree.tostring(html).decode())</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取class为item-1 li下的a的herf</span></span><br><span class="line">ret1 = html.xpath(<span class="string">&quot;//li[@class=&#x27;item-1&#x27;]/a/@href&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取class为item-1 li下的a的文本</span></span><br><span class="line">ret2 = html.xpath(<span class="string">&quot;//li[@class=&#x27;item-1&#x27;]/a/text()&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个li是一条新闻，把url和文本组成字典</span></span><br><span class="line"><span class="keyword">for</span> href <span class="keyword">in</span> ret1:</span><br><span class="line">    item = &#123;&#125;</span><br><span class="line">    item[<span class="string">&quot;href&quot;</span>] = href</span><br><span class="line">    item[<span class="string">&quot;title&quot;</span>] = ret2[ret1.index(href)]</span><br><span class="line">    <span class="built_in">print</span>(item)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#分组，根据li标签进行分组，对每一组继续写xpath</span></span><br><span class="line">ret3 = html.xpath(<span class="string">&quot;//li[@class=&#x27;item-1&#x27;]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret3)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> ret3:</span><br><span class="line">    item=  &#123;&#125;</span><br><span class="line">    item[<span class="string">&quot;title&quot;</span>] = i.xpath(<span class="string">&quot;a/text()&quot;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(i.xpath(<span class="string">&quot;./a/text()&quot;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    item[<span class="string">&quot;href&quot;</span>] = i.xpath(<span class="string">&quot;./a/@href&quot;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>( i.xpath(<span class="string">&quot;./a/@href&quot;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure><p>执行结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;&lt;body&gt;&lt;div&gt;&lt;ul&gt; </span><br><span class="line">    &lt;li class=&quot;item-1&quot;&gt;&lt;a&gt;first item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">    &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">    &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">    &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">    &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;</span><br><span class="line">    &lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br><span class="line">[&#x27;link2.html&#x27;, &#x27;link4.html&#x27;]</span><br><span class="line">[&#x27;first item&#x27;, &#x27;second item&#x27;, &#x27;fourth item&#x27;]</span><br><span class="line">&#123;&#x27;href&#x27;: &#x27;link2.html&#x27;, &#x27;title&#x27;: &#x27;first item&#x27;&#125;</span><br><span class="line">&#123;&#x27;href&#x27;: &#x27;link4.html&#x27;, &#x27;title&#x27;: &#x27;second item&#x27;&#125;</span><br><span class="line">****************************************************************************************************</span><br><span class="line">[&lt;Element li at 0x7f596c25cc88&gt;, &lt;Element li at 0x7f596c25cc48&gt;, &lt;Element li at 0x7f596c25ce08&gt;]</span><br><span class="line">&#123;&#x27;title&#x27;: &#x27;first item&#x27;, &#x27;href&#x27;: None&#125;</span><br><span class="line">&#123;&#x27;title&#x27;: &#x27;second item&#x27;, &#x27;href&#x27;: &#x27;link2.html&#x27;&#125;</span><br><span class="line">&#123;&#x27;title&#x27;: &#x27;fourth item&#x27;, &#x27;href&#x27;: &#x27;link4.html&#x27;&#125;</span><br></pre></td></tr></table></figure><p>lxml使用的<strong>注意点</strong>：</p><blockquote><p>lxml能够修正HTML代码，但是可能会改错了<br>我们可以先使用etree.tostring观察修改之后的html的样子<br>然后根据修改之后的html字符串写xpath</p><p>lxml能够接受bytes和str的字符串<br>提取页面数据的思路:</p><ul><li>先分组，取到一个包含分组标签的列表</li><li>然后遍历，取其中每一组数据，再进行提取，这样就不会造成数据的对应错乱</li></ul></blockquote><h3 id="爬虫套路"><a href="#爬虫套路" class="headerlink" title="爬虫套路"></a>爬虫套路</h3><ul><li><p><strong>准备url</strong></p><blockquote><p>当url地址规律不明显或者总数不确定的时候，我们就需要准备一个start_url<br>然后通过获取的响应代码提取下一页的url<br>如果url规律很明显，页码总数明确的话，我们就可以准备url_list</p></blockquote></li><li><p><strong>发送请求，获取响应</strong></p><blockquote><ul><li>添加随机的User-Agent,反反爬虫</li><li>添加随机的代理ip，反反爬虫</li><li>在对方判断出我们是爬虫之后，应该添加更多的headers字段，包括cookie</li><li>cookie的处理可以使用session来解决</li><li>准备一堆能用的cookie，组成cookie池<br>如果不登录就准备刚开始能够成功请求对方网站的cookie<br>即接收对方网站设置在response的cookie<br>下一次请求的时候，使用之前的列表中的cookie来请求<br>如果登录的话，就需要：<ul><li>准备多个账号</li><li>使用程序获取每个账号的cookie</li><li>之后请求登录之后才能访问的网站随机的选择cookie</li></ul></li></ul></blockquote></li><li><p><strong>提取数据</strong></p><blockquote><p>首先我们需要确定数据的位置</p><ul><li><p>如果数据在当前的url地址中<br>提取的是列表页的数据，就直接请求列表页的url地址，不用进入详情页<br>提取的是详情页的数据就需要：</p><ul><li><ol><li>确定url</li></ol></li><li><ol start="2"><li>发送请求</li></ol></li><li><ol start="3"><li>提取数据</li></ol></li><li><ol start="4"><li>返回</li></ol></li></ul></li><li><p>如果数据不在当前的url地址中<br>那么我们就要在其他的响应中，寻找数据的位置:</p><ul><li><ol><li>从network中从上往下找</li></ol></li><li><ol start="2"><li>使用chrome中的过滤条件，选择出了js,css,img之外的按钮</li></ol></li><li><ol start="3"><li>使用chrome的search all file，搜索数字和英文</li></ol></li></ul></li></ul><p>获取数据之后就要进行数据的提取</p><ul><li>xpath,从html中提取整块的数据，先分组，之后每一组再提取</li><li>re，提取max_time,price,html中的json字符串</li><li>json</li></ul></blockquote></li><li><p><strong>保存</strong></p><blockquote><p>可以选择保存在本地，如text,json,csv<br>也可以保存在数据库</p></blockquote></li></ul><h3 id="案例：贴吧爬虫"><a href="#案例：贴吧爬虫" class="headerlink" title="案例：贴吧爬虫"></a>案例：贴吧爬虫</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TiebaSpider</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, name</span>):</span><br><span class="line">        self.name = name</span><br><span class="line">        self.start_url = <span class="string">&quot;http://tieba.baidu.com/mo/q----,sz@320_240-1-3---2/m?kw=&quot;</span>+name+<span class="string">&quot;&amp;pn=0&quot;</span></span><br><span class="line">        self.part_url = <span class="string">&quot;http://tieba.baidu.com/mo/q----,sz@320_240-1-3---2/&quot;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;User-Agent&quot;</span>:</span><br><span class="line">            <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;发送请求，获取响应&quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(url)</span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> response.content</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_content_list</span>(<span class="params">self, html_str</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;提取数据&quot;&quot;&quot;</span></span><br><span class="line">        html = etree.HTML(html_str)</span><br><span class="line"></span><br><span class="line">        div_list = html.xpath(<span class="string">&quot;//div[contains(@class, &#x27;i&#x27;)]&quot;</span>)  <span class="comment"># 根据div进行分组</span></span><br><span class="line">        content_list = []</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[<span class="string">&quot;title&quot;</span>] = div.xpath(<span class="string">&#x27;./a/text()&#x27;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(div.xpath(<span class="string">&#x27;./a/text()&#x27;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            item[<span class="string">&quot;href&quot;</span>] = self.part_url+div.xpath(<span class="string">&#x27;./a/@href&#x27;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(div.xpath(<span class="string">&#x27;./a/@href&#x27;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            item[<span class="string">&quot;img_list&quot;</span>] = self.get_img_list(item[<span class="string">&#x27;href&#x27;</span>], [])</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&#x27;img_list&#x27;</span>])&gt;<span class="number">0</span>:</span><br><span class="line">                item[<span class="string">&quot;img_list&quot;</span>] = [requests.utils.unquote(i).split(<span class="string">&quot;src=&quot;</span>)[-<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> item[<span class="string">&#x27;img_list&#x27;</span>]]</span><br><span class="line">            content_list.append(item)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取下一页url地址</span></span><br><span class="line">        next_url = self.part_url + html.xpath(<span class="string">&#x27;//a[text()=&quot;下一页&quot;]/@href&#x27;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(html.xpath(<span class="string">&#x27;//a[text()=&quot;下一页&quot;]/@href&#x27;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> content_list, next_url</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_img_list</span>(<span class="params">self, detail_url, total_img_list</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;获取图片&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 1. 请求列表页的url地址，获取详情页的第一页</span></span><br><span class="line">        detail_html_str = self.parse_url(detail_url)</span><br><span class="line">        detail_html = etree.HTML(detail_html_str)</span><br><span class="line">        <span class="comment"># 2. 提取所有图片的URL和下一页的url</span></span><br><span class="line">        img_url = detail_html.xpath(<span class="string">&#x27;//img[@class=&quot;BDE_Image&quot;]/@src&#x27;</span>)[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(detail_html.xpath(<span class="string">&#x27;//img[@class=&quot;BDE_Image&quot;]/@src&#x27;</span>))&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> img_url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            total_img_list.append(img_url)</span><br><span class="line">        detail_next_url = detail_html.xpath(<span class="string">&#x27;//a[text()=&quot;下一页&quot;]/@href&#x27;</span>)</span><br><span class="line">        <span class="comment"># 3. 请求下一页，进入循环(此处用递归)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(detail_next_url)&gt;<span class="number">0</span>:</span><br><span class="line">            detail_next_url = self.part_url + detail_next_url[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">return</span> self.get_img_list(detail_next_url, total_img_list)</span><br><span class="line">        <span class="comment"># 所有数据获取完毕后跳出递归</span></span><br><span class="line">        <span class="keyword">return</span> total_img_list</span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_content_list</span>(<span class="params">self, content_list</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;保存数据&quot;&quot;&quot;</span></span><br><span class="line">        file_path = self.name + <span class="string">&quot;.txt&quot;</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;a&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> content <span class="keyword">in</span> content_list:</span><br><span class="line">                f.write(json.dumps(content, ensure_ascii=<span class="literal">False</span>, indent=<span class="number">2</span>))</span><br><span class="line">                f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;保存成功&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 1. 构造start_url</span></span><br><span class="line">        url = self.start_url</span><br><span class="line">        <span class="keyword">while</span> url <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 2. 发送请求，获取响应</span></span><br><span class="line">            html_str = self.parse_url(url)</span><br><span class="line">            <span class="comment"># 3. 提取数据，构造下一页url地址</span></span><br><span class="line">            content_list, url = self.get_content_list(html_str)</span><br><span class="line">            <span class="comment"># 4. 保存数据</span></span><br><span class="line">            self.save_content_list(content_list)</span><br><span class="line">            <span class="comment"># 5. 请求下一页地址，进入循环</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    tieba_spider = TiebaSpider(<span class="string">&#x27;vim&#x27;</span>)</span><br><span class="line">    tieba_spider.run()</span><br></pre></td></tr></table></figure><h3 id="多线程爬虫"><a href="#多线程爬虫" class="headerlink" title="多线程爬虫"></a>多线程爬虫</h3><p>我们可以使用队列来让多个线程之间相互通信<br>因此这个多线程爬虫注意是利用队列来进行数据在不同线程中的共享</p><blockquote><p>Queue的<strong>注意点</strong>:<br>Queue使用put方法的时候会让计数加一，但是使用get方法取元素的时候计数并不会减一<br>get方法需要和<strong>task_done</strong>方法在一起的时候才会让计数减一<br>Queue的join方法可以让主线程等待阻塞，等待队列的任务完成之后再完成</p></blockquote><p>多线程爬虫案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> Queue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QiubaiSpider</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url_temp = <span class="string">&quot;https://www.qiushibaike.com/8hr/page/&#123;&#125;/&quot;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;User-Agent&quot;</span>:</span><br><span class="line">            <span class="string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        self.url_queue = Queue()</span><br><span class="line">        self.html_queue = Queue()</span><br><span class="line">        self.content_queue = Queue()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_url_list</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">            self.url_queue.put(self.url_temp.<span class="built_in">format</span>(i))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_url</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            url = self.url_queue.get()</span><br><span class="line"></span><br><span class="line">            <span class="comment">#print(url)</span></span><br><span class="line">            response = requests.get(url, headers=self.headers)</span><br><span class="line">            self.html_queue.put(response.content.decode())</span><br><span class="line">            self.url_queue.task_done()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_content_list</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            html_str = self.html_queue.get()</span><br><span class="line">            html = etree.HTML(html_str)</span><br><span class="line">            div_list = html.xpath(<span class="string">&quot;//div[@id=&#x27;content-left&#x27;]/div&quot;</span>)</span><br><span class="line">            content_list = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">                item = &#123;&#125;</span><br><span class="line">                item[<span class="string">&quot;content&quot;</span>] = div.xpath(<span class="string">&quot;.//div[@class=&#x27;content&#x27;]/span/text()&quot;</span>)</span><br><span class="line">                item[<span class="string">&quot;content&quot;</span>] = [i.replace(<span class="string">&quot;\n&quot;</span>,<span class="string">&quot;&quot;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> item[<span class="string">&quot;content&quot;</span>]]</span><br><span class="line">                item[<span class="string">&quot;author_gender&quot;</span>] = div.xpath(<span class="string">&quot;.//div[contains(@class,&#x27;articleGender&#x27;)]/@class&quot;</span>)</span><br><span class="line">                item[<span class="string">&quot;author_gender&quot;</span>] = item[<span class="string">&quot;author_gender&quot;</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[-<span class="number">1</span>].replace(<span class="string">&quot;Icon&quot;</span>,<span class="string">&quot;&quot;</span>) <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&quot;author_gender&quot;</span>])&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                item[<span class="string">&quot;auhtor_age&quot;</span>] = div.xpath(<span class="string">&quot;.//div[contains(@class,&#x27;articleGender&#x27;)]/text()&quot;</span>)</span><br><span class="line">                item[<span class="string">&quot;auhtor_age&quot;</span>] = item[<span class="string">&quot;auhtor_age&quot;</span>][<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&quot;auhtor_age&quot;</span>])&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                item[<span class="string">&quot;content_img&quot;</span>] = div.xpath(<span class="string">&quot;.//div[@class=&#x27;thumb&#x27;]/a/img/@src&quot;</span>)</span><br><span class="line">                item[<span class="string">&quot;content_img&quot;</span>] = <span class="string">&quot;https:&quot;</span>+item[<span class="string">&quot;content_img&quot;</span>][<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&quot;content_img&quot;</span>])&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                item[<span class="string">&quot;author_img&quot;</span>] = div.xpath(<span class="string">&quot;.//div[@class=&#x27;author clearfix&#x27;]//img/@src&quot;</span>)</span><br><span class="line">                item[<span class="string">&quot;author_img&quot;</span>] = <span class="string">&quot;https:&quot;</span>+item[<span class="string">&quot;author_img&quot;</span>][<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&quot;author_img&quot;</span>])&gt;<span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                item[<span class="string">&quot;stats_vote&quot;</span>] = div.xpath(<span class="string">&quot;.//span[@class=&#x27;stats-vote&#x27;]/i/text()&quot;</span>)</span><br><span class="line">                item[<span class="string">&quot;stats_vote&quot;</span>] = item[<span class="string">&quot;stats_vote&quot;</span>][<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&quot;stats_vote&quot;</span>])&gt;<span class="number">0</span> <span class="keyword">else</span> Non</span><br><span class="line">                content_list.append(item)</span><br><span class="line"></span><br><span class="line">            self.content_queue.put(content_list)</span><br><span class="line">            self.html_queue.task_done()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_content_list</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            content_list = self.content_queue.get()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> content_list:</span><br><span class="line">                <span class="built_in">print</span>(i)</span><br><span class="line">            self.content_queue.task_done()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        thread_list = []</span><br><span class="line">        t_url = threading.Thread(target=self.get_url_list)</span><br><span class="line">        t_parse = threading.Thread(target=self.parse_url)</span><br><span class="line">        t_html = threading.Thread(target=self.get_content_list)</span><br><span class="line">        t_save = threading.Thread(target=self.save_content_list)</span><br><span class="line">        thread_list.append(t_url)</span><br><span class="line">        thread_list.append(t_parse)</span><br><span class="line">        thread_list.append(t_html)</span><br><span class="line">        thread_list.append(t_save)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> thread_list:</span><br><span class="line">            t.setDaemon(<span class="literal">True</span>)  <span class="comment"># 将子线程设置为守护进程，主线程结束后该线程结束</span></span><br><span class="line">            t.start()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> q <span class="keyword">in</span> [self.url_queue,self.html_queue,self.content_queue]:</span><br><span class="line">            <span class="comment"># 等待队列使用完毕</span></span><br><span class="line">            q.join()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    qiubai = QiubaiSpider()</span><br><span class="line">    qiubai.run() </span><br></pre></td></tr></table></figure><h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><blockquote><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的<strong>应用框架</strong><br>我们只需要实现少量的代码，就能够快速的抓取<br>Scrapy使用了Twisted异步网络框架，可以加快我们的下载速度。</p></blockquote><h3 id="Scrapy-流程"><a href="#Scrapy-流程" class="headerlink" title="Scrapy 流程"></a>Scrapy 流程</h3><p>我们上面所学的爬虫流程如下：</p><p>Scrapy 框架就是一个爬虫框架，因此它的流程和爬虫流程差不多，如下：</p><p>其中的内容如下：</p><table><thead><tr><th align="center">结构</th><th>作用</th><th align="center">是否需要实现</th></tr></thead><tbody><tr><td align="center">Scrapy Engine(引擎)</td><td>总指挥：负责数据和信号在不同模块间的传递</td><td align="center">Scrapy已实现</td></tr><tr><td align="center">Scheduler(调度器)</td><td>一个队列，存放引擎发送过来的Request请求</td><td align="center">Scrapy已实现</td></tr><tr><td align="center">Downloader(下载器)</td><td>下载引擎发送过来的Request请求，将结果返回给引擎</td><td align="center">Scrapy已实现</td></tr><tr><td align="center">Spider(爬虫)</td><td>处理引擎发送的response，提取数据和URL，并返回给引擎</td><td align="center">需要手写</td></tr><tr><td align="center">Item Pipeline(管道)</td><td>处理引擎发送过来的数据，比如存储</td><td align="center">需要手写</td></tr><tr><td align="center">Downloader Middlewares(下载中间件)</td><td>可以自定义的下载扩展，比如设置代理</td><td align="center">一般不用手写</td></tr><tr><td align="center">Spider Middlewares(爬虫中间件)</td><td>可以自定义requests请求和进行response过滤</td><td align="center">一般不用手写</td></tr></tbody></table><h3 id="Scrapy-入门"><a href="#Scrapy-入门" class="headerlink" title="Scrapy 入门"></a>Scrapy 入门</h3><p>要使用 Scrapy 框架，首先需要下载这个框架，这里不予说明<br>创建一个 Scrapy 项目需要使用 scrapy 命令，如下：<br><code>scrapy startproject mySpider</code><br>然后进入项目目录中生成一个爬虫，如下：<br><code>scrapy genspider NAME &quot;NAME.com/cn&quot;</code><br>这样，一个scrapy框架就完成了</p><p>例如我这里使用<code>scrapy startproject itcastSpider</code>创建一个爬虫项目，<br>然后使用<code>scrapy genspider itcast itcast.cn</code>生成一个爬虫，目录结构如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── itcastSpider</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py        ---&gt; 自己预计需要爬取的内容</span><br><span class="line">│   ├── middlewares.py  ---&gt; 自定义中间件</span><br><span class="line">│   ├── pipelines.py    ---&gt; 管道，保存数据</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   ├── settings.py     ---&gt; 设置文件，UA，启动管道</span><br><span class="line">│   └── spiders         ---&gt; 自己定义的爬虫文件夹</span><br><span class="line">│       ├── __init__.py</span><br><span class="line">│       ├── itcast.py</span><br><span class="line">│       └── __pycache__</span><br><span class="line">└── scrapy.cfg          ---&gt; 项目的配置文件</span><br></pre></td></tr></table></figure><p>首先编辑spider中的爬虫文件，内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ItcastSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;itcast&#x27;</span>  <span class="comment"># 爬虫名</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;itcast.cn&#x27;</span>]  <span class="comment"># 允许爬取的范围</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.itcast.cn/channel/teacher.shtml&#x27;</span>]  <span class="comment"># 最开始请求的url</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;处理start_url地址对应的响应&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#ret = response.xpath(&#x27;//div[@class=&quot;tea_con&quot;]//h3/text()&#x27;).extract()</span></span><br><span class="line">        <span class="comment">#print(ret)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分组</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;tea_con&quot;]//li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[<span class="string">&quot;name&quot;</span>] = li.xpath(<span class="string">&#x27;.//h3/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&quot;title&quot;</span>] = li.xpath(<span class="string">&#x27;.//h4/text()&#x27;</span>).extract_first()</span><br><span class="line">            <span class="comment">#print(item)</span></span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><p><strong>注意:</strong></p><blockquote><p>爬虫类中的name值需要和使用<code>spider genspider</code>命令中定义的爬虫名一致<br>allowed_domains值需要和命令中的域名一致<br>parse方法是必须要有的方法，其中进行爬取操作<br>response对象是一个响应对象，可以对其进行xpath操作，会得到<strong>Selector</strong>对象<br>如果要获取Selector对象的值，需要调用<strong>extract</strong>方法获取其中的内容，该方法返回一个包含有字符串数据的列表<br>但是如果得到的Selector对象中没有内容，extract方法会抛异常，因此常常使用<strong>extract_first</strong>方法，该方法返回列表中的第一个字符串<br>该方法如果没有获取值不会抛异常，只会将值置为None<br>爬虫中的parse方法可以使用<strong>yield</strong>将函数变成一个<strong>生成器</strong>，可以将数据传到Pipeline中<br>爬虫使用yield之后，可以直接在Pipeline中使用这个yield的值了(<strong>该值在Pipeline中用item接收</strong>)<br>只能yield这几种：<code>Request, BaseItem, dict or None</code></p></blockquote><p>Pipeline文件如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpiderPipeline1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        item[<span class="string">&quot;hello&quot;</span>] = <span class="string">&quot;world&quot;</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpiderPipeline2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><p>写完Pipeline文件之后，需要在setting文件中启动它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;itcastSpider.pipelines.MySpiderPipeline1&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;itcastSpider.pipelines.MySpiderPipeline2&#x27;</span>: <span class="number">301</span>,</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><blockquote><p>setting文件中的ITEM_PIPELINES字典中的键为<strong>pipeline的位置</strong>，值为<strong>权重</strong><br>如果有多个Pipeline，则元素会先到权重小的管道中，然后再到权重高的管道中<br>如这个爬虫的执行结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;潘老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>: <span class="string">&#x27;world&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;胡老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;胡老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>: <span class="string">&#x27;world&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;贡老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;贡老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>: <span class="string">&#x27;world&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;王老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;王老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>: <span class="string">&#x27;world&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;肖老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;肖老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>: <span class="string">&#x27;world&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;信老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>&#125;</span><br><span class="line">&gt;&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;信老师&#x27;</span>, <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;高级讲师&#x27;</span>, <span class="string">&#x27;hello&#x27;</span>: <span class="string">&#x27;world&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p>可见，每个元素先到300的管道中，添加了一个键值对后再到301的管道中<br><strong>注意</strong>：pipeline中的process_item方法名不能修改</p></blockquote><h3 id="设置-log"><a href="#设置-log" class="headerlink" title="设置 log"></a>设置 log</h3><p>为了让我们自己希望输出到终端的内容能容易看一些：<br>我们可以在setting中设置<strong>log级别</strong><br>在setting中添加一行（全部大写）：<code>LOG_LEVEL = &quot;WARNING&quot;</code></p><p>关于log日志，有一个模块可以用来输出log日志信息，该模块就是<code>logging</code><br>我们可以自己通过logging模块写一个log模块，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置日志的输出格式</span></span><br><span class="line">logging.basicConfig(level=logging.INFO,</span><br><span class="line">        <span class="built_in">format</span>=<span class="string">&#x27;[%(asctime)s] &#x27;</span></span><br><span class="line">        <span class="string">&#x27;%(levelname)s [%(filename)s:%(lineno)d] &#x27;</span></span><br><span class="line">        <span class="string">&#x27;: %(message)s&#x27;</span>, </span><br><span class="line">        datefmt=<span class="string">&#x27;%d/%b/%Y %H:%M:%S&#x27;</span>,)</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    logger.info(<span class="string">&quot;This is a info log&quot;</span>)</span><br><span class="line">    logger.info(<span class="string">&quot;This is a info log 1&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[21/Mar/2019 12:48:09] INFO [log.py:15] : This is a info log</span><br><span class="line">[21/Mar/2019 12:48:09] INFO [log.py:16] : This is a info log 1</span><br></pre></td></tr></table></figure><p>这个模块定义完之后，可以在代码中使用这个模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- encoding: utf-8 -*- </span></span><br><span class="line"><span class="keyword">from</span> log <span class="keyword">import</span> logger</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    logger.warning(<span class="string">&quot;This is a log 1&quot;</span>)</span><br><span class="line">    logger.warning(<span class="string">&quot;This is a log 2&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[21/Mar/2019 12:49:20] WARNING [log_print.py:6] : This is a log 1</span><br><span class="line">[21/Mar/2019 12:49:20] WARNING [log_print.py:7] : This is a log 2</span><br></pre></td></tr></table></figure><blockquote><p>log模块关键是要通过getLogger方法获取模块名来实例化一个logging对象<br>如果没有获取__name__就无法输出log信息的位置</p></blockquote><h3 id="scrapy-Request"><a href="#scrapy-Request" class="headerlink" title="scrapy.Request"></a>scrapy.Request</h3><p>如果我们获取到一个URL，那么如何再次发送请求呢？<br>scrapy.Request能构建一个requests，可以用来发送请求<br>如下：<br><code>scrapy.Request(url [, callback, method=&#39;GET&#39;, headers, body, cookies, meta, dont_filter=False])</code><br>其中常用的参数为：</p><ul><li><strong>callback</strong>：指定传入的URL交给哪个解析函数去处理</li><li><strong>meta</strong>：实现在不同的解析函数中传递数据，meta默认会携带部分信息</li><li><strong>dont_filter</strong>：让scrapy的去重不会过滤当前的URL，scrapy默认有URL去重的功能，对需要重复请求的URL有重要用途</li></ul><p>例如使用Request实现翻页功能(爬虫parse其他的功能实现完毕)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">next_url = response.xpath(<span class="string">&#x27;//a[test()=&quot;下一页&quot;]/@href&#x27;</span>).extract_first()</span><br><span class="line"><span class="keyword">if</span> next_url:</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(next_url, callback=self.parse)</span><br></pre></td></tr></table></figure><p>又如使用Request请求子页面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    tr_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;greyframe&quot;]/table//table/tr&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tr_list:</span><br><span class="line">        item = Item()</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url, callback=self.get_content, meta=&#123;<span class="string">&quot;item&quot;</span>:item&#125;)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">self, response</span>):</span><br><span class="line">    item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">    item[<span class="string">&#x27;content&#x27;</span>] = response.xpath(<span class="string">&#x27;//div[@class=&quot;content&quot;]//text()&#x27;</span>).extract()</span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h3 id="Item"><a href="#Item" class="headerlink" title="Item"></a>Item</h3><p>可以把Item理解成一个字典，和我们平常使用的字典一样，在爬虫中可以yield这个Item<br>在Item.py文件中可以定义它的<strong>键</strong>，需要使用scrapy.Field()方法来定义这个键<br>如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TencentItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    position = scrapy.Field()</span><br><span class="line">    publish_date = scrapy.Field()</span><br></pre></td></tr></table></figure><p>然后在我们的爬虫文件中需要先实例化这个Item，才能给这个Item的键赋值，赋值方法和字典一样，唯一不同的是<strong>如果item中没有这个键，就会报错</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> tencent.items <span class="keyword">import</span> TencentItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HrSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;hr&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;tencent.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://hr.tencent.com/position.php&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        tr_list = response.xpath(<span class="string">&quot;//table[@class=&#x27;tablelist&#x27;]/tr&quot;</span>)[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> tr_list:</span><br><span class="line">            <span class="comment"># 实例化我们定义好的Item</span></span><br><span class="line">            item = TencentItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = tr.xpath(<span class="string">&#x27;./td[1]/a/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;position&#x27;</span>] = tr.xpath(<span class="string">&#x27;.//td[2]/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;publish_date&#x27;</span>] = tr.xpath(<span class="string">&#x27;./td[5]/text()&#x27;</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到下一页的URL地址</span></span><br><span class="line">        next_url = response.xpath(<span class="string">&#x27;//a[@id=&quot;next&quot;]/@href&#x27;</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> next_url != <span class="string">&#x27;javascripy:;&#x27;</span>:</span><br><span class="line">            next_url = <span class="string">&#x27;http://hr.tencent.com/&#x27;</span> + next_url</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                <span class="comment"># 由于请求下一页的URL地址和这一页使用的方式是一样的，因此调用自身</span></span><br><span class="line">                callback=self.parse</span><br><span class="line">            )</span><br></pre></td></tr></table></figure><p>在把数据交给pipeline的时候可以通过<code>isinstance(item, ITEMNAME)</code>来判断数据是属于哪个item，用于进行不同的处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> tencent.items <span class="keyword">import</span> TencentItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TencentPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(item, TencentItem):</span><br><span class="line">            <span class="built_in">print</span>(item)</span><br><span class="line">            <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h3 id="scrapy-shell"><a href="#scrapy-shell" class="headerlink" title="scrapy shell"></a>scrapy shell</h3><p><strong>Scrapy shell</strong>是一个交互终端，我们可以在<strong>未启动spider的情况下</strong>尝试及调试代码<br>也可以用来测试XPath表达式</p><p>使用方法：<br><code>scrapy shell URL</code></p><p>常用的属性:</p><ul><li>response.url：当前响应的url地址</li><li>response.request.url：当前响应对应的请求的url地址</li><li>response.headers：响应头</li><li>response.body：响应体，也就是html代码，默认是byte类型</li><li>response.requests.headers：当前响应的请求头</li></ul><h3 id="setting"><a href="#setting" class="headerlink" title="setting"></a>setting</h3><p>setting.py文件是一个配置文件，存放一些公共的变量(比如数据库的地址等)<br>使用配置文件可以方便自己和其他人进行修改<br>在配置文件中一般使用全大写字母命名变量名<br>在setting.py文件中定义了变量之后可以在爬虫的setting属性中找到，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Spider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    allowed_domains = []</span><br><span class="line">    start_urls = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 获取settings文件中定义的变量</span></span><br><span class="line">        self.settings[<span class="string">&quot;MONGO_HOST&quot;</span>]</span><br><span class="line">        self.settings.get(<span class="string">&quot;MONGO_HOST&quot;</span>,<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>在pipeline中直接通过传入的spider参数调用即可</p><h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><p>在爬虫开启的时候会执行一个open_spider方法，仅执行一次<br>与之对应的是在爬虫关闭的时候会执行一个close_spider方法，也仅执行一次<br>我们可以在这两个方法中添加一些属性等，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Pipeline</span>(<span class="title class_ inherited__">opject</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.file = <span class="built_in">open</span>(spider.settings.get(<span class="string">&quot;SAVE_FILE&quot;</span>, <span class="string">&quot;./temp.txt&quot;</span>), <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h3 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h3><p>CrawlSpider是scrapy框架中的一个继承自spiders的类<br>其中有一些特殊的功能能帮助我们减少代码量<br>用命令创建一个crawlspider的模板：<br><code>scrapy genspider -t crawl NAME ALLOW_DOMAIN</code><br>创建好之后的CrawlSpider内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TtSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;\Items&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>Rule中的LinkExtractor是一个连接提取器，其常用的参数如下：</p><ul><li><strong>allow</strong>：满足括号中正则表达式的URL会被提取，如果为空则全部匹配</li><li><strong>deny</strong>：满足括号中正则表达式的URL不提取(优先级高于allow)</li><li><strong>restrict_xpaths</strong>：使用xpath表达式，和allow共同作用</li></ul><p>scrapy.Rule常见参数如下：</p><ul><li><strong>link_extractor</strong>: 一个LinkExtractor对象，用于定义需要提取的链接</li><li><strong>callback</strong>：从LinkExtractor中获取到链接时所调用的方法</li><li><strong>follow</strong>：指定根据该规则从response提取的链接是否需要跟进</li><li><strong>process_links</strong>：指定该spider中哪个函数将会被调用，从link_extractor中获取的链接列表时将会调用该函数，<strong>该方法主要用来过滤URL</strong></li><li><strong>process_request</strong>：指定该spider中哪个函数将会被调用，该规则提取到每个request时都调用该函数，<strong>主要用来过滤request</strong></li></ul><p><strong>注意</strong>：</p><blockquote><p>CrawlSpider中不能再有以parse为名字的数据提取方法，这个方法被用来实现基础url提取等功能</p></blockquote><p>案例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChufaSpider</span>(<span class="title class_ inherited__">CrawlSpider</span>):</span><br><span class="line">    name = <span class="string">&#x27;chufa&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;circ.gov.cn&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://bxjg.circ.gov.cn/web/site0/tab5240/module14430/page1.htm&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义提取URL地址规则</span></span><br><span class="line">    rules = ( </span><br><span class="line">        <span class="comment"># LinkExtractor 连接提取器，提取URL地址</span></span><br><span class="line">        <span class="comment"># callback 提取出来的URL地址的response会交给callback函数处理</span></span><br><span class="line">        <span class="comment"># follow 当前URL地址的响应是否重新经过rules来提取url地址</span></span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;/web/site0/tab5240/info\d+\.htm&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;/web/site0/tab5240/module14430/page\d+\.htm&#x27;</span>), follow=<span class="literal">True</span>),</span><br><span class="line">    ) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse函数有特殊功能，不能定义</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_item</span>(<span class="params">self, response</span>):</span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        item[<span class="string">&#x27;title&#x27;</span>] = re.findall(<span class="string">r&#x27;&lt;!--TitleStart--&gt;(.*?)&lt;!--TitleEnd--&gt;&#x27;</span>, response.body.decode())[<span class="number">0</span>]</span><br><span class="line">        item[<span class="string">&#x27;publish_date&#x27;</span>] = re.findall(<span class="string">r&#x27;发布时间：(20\d&#123;2&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&#x27;</span>, response.body.decode())[<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure><p>该案例中的Rule一个用于获取子页面URL进行请求，另一个用于翻页</p><p><strong>注意点</strong>：</p><blockquote><ul><li>url地址不完整，crawlspider会自动补充完整之后在请求</li><li>parse函数不能定义，他有特殊的功能需要实现</li><li>callback：连接提取器提取出来的url地址对应的响应交给他处理</li><li>follow：连接提取器提取出来的url地址对应的响应是否继续被rules来过滤</li></ul></blockquote></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Janvy Sun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://janvysun.github.io/python/2eb6c6ca-e6ee-40b2-8aab-df84770e2b79.html">https://janvysun.github.io/python/2eb6c6ca-e6ee-40b2-8aab-df84770e2b79.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://janvysun.github.io" target="_blank">Janvy's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="/images/hexo.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/maven/41ecc68f-93a4-4de1-aae2-0e48a6d506fa.html"><img class="prev-cover" src="/images/hexo.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Maven 的安装与配置</div></div></a></div><div class="next-post pull-right"><a href="/python/8541a389-77d7-45ce-a30a-187778c6725a.html"><img class="next-cover" src="/images/hexo.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Python 进阶</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/python/79f3fe31-f5c4-425b-9cd2-51993efc7e10.html" title="Python 入门"><img class="cover" src="/images/hexo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-25</div><div class="title">Python 入门</div></div></a></div><div><a href="/python/cbbc7563-8281-4bcc-a956-7a976dbf2643.html" title="Python 字符串"><img class="cover" src="/images/hexo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-28</div><div class="title">Python 字符串</div></div></a></div><div><a href="/python/894d516f-7aa7-4c9a-9060-6f2bbe95af74.html" title="Python 输入输出"><img class="cover" src="/images/hexo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-27</div><div class="title">Python 输入输出</div></div></a></div><div><a href="/python/1827654a-05d2-4fda-8b4f-18279065c88d.html" title="Python 字典"><img class="cover" src="/images/hexo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-01</div><div class="title">Python 字典</div></div></a></div><div><a href="/python/20125780-f44a-432b-868b-99d90124f74f.html" title="Python 列表、元组与集合"><img class="cover" src="/images/hexo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-29</div><div class="title">Python 列表、元组与集合</div></div></a></div><div><a href="/python/fedbf4b8-90d9-4204-a3bb-392957dd9e4b.html" title="Python 变量"><img class="cover" src="/images/hexo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-26</div><div class="title">Python 变量</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">Janvy Sun</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">153</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">47</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/janvysun" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:janvusun@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">爬虫基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">爬虫的流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests-%E6%A8%A1%E5%9D%97"><span class="toc-number">2.</span> <span class="toc-text">Requests 模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Requests-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">2.1.</span> <span class="toc-text">Requests 基本操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E9%80%81%E5%B8%A6-header-%E7%9A%84%E8%AF%B7%E6%B1%82"><span class="toc-number">2.2.</span> <span class="toc-text">发送带 header 的请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E9%80%81%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E8%AF%B7%E6%B1%82"><span class="toc-number">2.3.</span> <span class="toc-text">发送带参数的请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E9%80%81-POST-%E8%AF%B7%E6%B1%82"><span class="toc-number">2.4.</span> <span class="toc-text">发送 POST 请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86"><span class="toc-number">2.5.</span> <span class="toc-text">使用代理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cookies-%E5%92%8C-Session"><span class="toc-number">2.6.</span> <span class="toc-text">Cookies 和 Session</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#requests-%E6%A8%A1%E5%9D%97%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7"><span class="toc-number">2.7.</span> <span class="toc-text">requests 模块的小技巧</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%88%B6%E8%AF%B7%E6%B1%82%E6%A8%A1%E5%9D%97"><span class="toc-number">2.8.</span> <span class="toc-text">自制请求模块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96"><span class="toc-number">3.</span> <span class="toc-text">数据提取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON"><span class="toc-number">3.1.</span> <span class="toc-text">JSON</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-number">3.2.</span> <span class="toc-text">正则表达式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xpath"><span class="toc-number">3.3.</span> <span class="toc-text">xpath</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lxml"><span class="toc-number">3.4.</span> <span class="toc-text">lxml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E5%A5%97%E8%B7%AF"><span class="toc-number">3.5.</span> <span class="toc-text">爬虫套路</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%EF%BC%9A%E8%B4%B4%E5%90%A7%E7%88%AC%E8%99%AB"><span class="toc-number">3.6.</span> <span class="toc-text">案例：贴吧爬虫</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB"><span class="toc-number">3.7.</span> <span class="toc-text">多线程爬虫</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy"><span class="toc-number">4.</span> <span class="toc-text">Scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">Scrapy 流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-%E5%85%A5%E9%97%A8"><span class="toc-number">4.2.</span> <span class="toc-text">Scrapy 入门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE-log"><span class="toc-number">4.3.</span> <span class="toc-text">设置 log</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy-Request"><span class="toc-number">4.4.</span> <span class="toc-text">scrapy.Request</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Item"><span class="toc-number">4.5.</span> <span class="toc-text">Item</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy-shell"><span class="toc-number">4.6.</span> <span class="toc-text">scrapy shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#setting"><span class="toc-number">4.7.</span> <span class="toc-text">setting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pipeline"><span class="toc-number">4.8.</span> <span class="toc-text">pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrawlSpider"><span class="toc-number">4.9.</span> <span class="toc-text">CrawlSpider</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/lyrics/1f1e25e2-0756-4881-9cc2-b146d03c0ae6.html" title="世界が終るまでは…"><img src="/images/hexo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="世界が終るまでは…"></a><div class="content"><a class="title" href="/lyrics/1f1e25e2-0756-4881-9cc2-b146d03c0ae6.html" title="世界が終るまでは…">世界が終るまでは…</a><time datetime="2024-04-20T16:00:00.000Z" title="发表于 2024-04-21 00:00:00">2024-04-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/games/bdac0118-3bf7-43f9-9856-142e616b1ca4.html" title="我的世界攻略"><img src="/images/hexo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="我的世界攻略"></a><div class="content"><a class="title" href="/games/bdac0118-3bf7-43f9-9856-142e616b1ca4.html" title="我的世界攻略">我的世界攻略</a><time datetime="2024-03-31T16:00:00.000Z" title="发表于 2024-04-01 00:00:00">2024-04-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/elk/3d7c2057-e448-4b83-a185-8d98e973490e.html" title="Nginx 的安装与使用"><img src="/images/hexo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Nginx 的安装与使用"></a><div class="content"><a class="title" href="/elk/3d7c2057-e448-4b83-a185-8d98e973490e.html" title="Nginx 的安装与使用">Nginx 的安装与使用</a><time datetime="2024-03-10T16:00:00.000Z" title="发表于 2024-03-11 00:00:00">2024-03-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/games/967c02f7-0d5e-4dd8-a2e0-70c97840fc5d.html" title="真三国无双 7 攻略"><img src="/images/hexo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="真三国无双 7 攻略"></a><div class="content"><a class="title" href="/games/967c02f7-0d5e-4dd8-a2e0-70c97840fc5d.html" title="真三国无双 7 攻略">真三国无双 7 攻略</a><time datetime="2024-02-29T16:00:00.000Z" title="发表于 2024-03-01 00:00:00">2024-03-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/docker/4c765195-289e-4cc4-88fb-12d9bb5fed61.html" title="Docker Compose 使用"><img src="/images/hexo.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Docker Compose 使用"></a><div class="content"><a class="title" href="/docker/4c765195-289e-4cc4-88fb-12d9bb5fed61.html" title="Docker Compose 使用">Docker Compose 使用</a><time datetime="2023-12-31T16:00:00.000Z" title="发表于 2024-01-01 00:00:00">2024-01-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2024 By Janvy Sun</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>